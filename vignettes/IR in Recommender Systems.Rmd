---
title: "Information Retrieval with Recommendation Systems"
author: "Mark Ewing"
date: "October 10, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(irmetrics)
#http://www.claudiobellei.com/2017/06/18/information-retrieval/
```

## Information Retrieval Metrics applied to Recommender Systems

This document is intended to serve as a tutorial for using the `irmetrics` package for evaluating recommender systems with information retrieval metrics.

We will assume that we have a collection of users each of whom receives a set of recommendations. The number of recommendations received by each person can vary. Additionally, we know (for the purposes of the tutorial) which products a user is acutally interested in.

If a recommendation is relevant, it will receive a relevance score of 1, 0 otherwise. Consider the case below. The numbers being recommended (and the numbers that are relevant) can be thought of as id values to some other database.

```{r testcase}
calculate_relevance = function(user){
  user$relevance = as.numeric(user$recommendations %in% user$interests)
  return(user)
}

user1 = list(
  recommendations = c(1,5,9),
  interests = c(1,2,3,4,5,6)
)
user1 = calculate_relevance(user1)
print(user1)
```

We see the first two elements recommended (1 and 5) are in the vector of the users interests, so they receive a value of 1 while the last element recommended (9) is not of interest so it receives a 0. Several more test cases are provided below.

```{r testcases}
user2 = calculate_relevance(list(
  recommendations = c(1,2,4,7,10),
  interests = c(4,7,9)
))

user3 = calculate_relevance(list(
  recommendations = c(),
  interests = c(3,6,7,8,9)
))

user4 = calculate_relevance(list(
  recommendations = c(2,3,6,8,10),
  interests = c()
))

user5 = calculate_relevance(list(
  recommendations = c(),
  interests = c()
))
```

### Precision@k

Precision is a measure of the probability that a recommendation is relevant to the user. The *k* part of it has to do with the order the recommendations are provided in. In this case we can think of it as summing up the first *k* elements of the *relevance* vector and dividing by k.

In our first test case, the first recommendation (1) was in the set of interests, so the precision is 1.

```{r}
P(user1$relevance,1)
```

When we look at a *k* of 3, the value drops to 2/3 because only 2 of the 3 recommendations were relevant.

```{r}
P(user1$relevance,3)
```

How is a value of *k* > `length(user$relevance)` handled? Consider a *k* of 5. In this case, we append missing values onto the relevance vector to match the length. This results in a P@5 value of 0.4 (2 out of 5) and a residual of 0.4 (2 missing values out of 5).

```{r}
P(user1$relevance,5)
```

Table of results for each user below.

```{r,echo=FALSE,warning=FALSE}
users = rep(list(user1,user2,user3,user4,user5),3)
users = lapply(users,`[[`,"relevance")
k = sort(rep(c(1,3,5),5))
varied_p = purrr::map2(users,k,P)
varied_p = vapply(varied_p,function(x){x[[1]]},FUN.VALUE=0)
p_table = matrix(varied_p,nrow=5)
colnames(p_table) = c("k=1","k=3","k=5")
rownames(p_table) = paste("User #",1:5,sep="")
knitr::kable(p_table, caption="P@k")
```